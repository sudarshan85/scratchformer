{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80413c68-2e7c-4786-aa86-7812201e505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import sys, os, time, warnings, pdb, pickle, random, math, re, json\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.insert(0, '../scripts')\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761cfc7-76ed-4579-81ae-2a36a1d31614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60b034-e50c-4c48-aebf-eb6ea599561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputEmbeddings, PositionalEncoding\n",
    "from input import *\n",
    "from internal import LayerNormalization, FeedForwardBlock, ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d3530-7b38-4731-85ee-98aa30a1a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "vocab_size = 8\n",
    "sos,eos,pad=1,2,3\n",
    "seq_len = 10\n",
    "dropout = 0.1\n",
    "d_ff = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b2ec8-2d59-46ce-b460-a41bd3df2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[sos,4,6,7,6,4,pad,pad,pad,eos], [sos,4,5,5,7,7,5,7,pad,eos]])\n",
    "emb = InputEmbeddings(d_model, vocab_size)\n",
    "pe = PositionalEncoding(d_model, seq_len, dropout)\n",
    "norm = LayerNormalization()\n",
    "ffb = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "residual_connection = ResidualConnection(dropout)\n",
    "residual_connection.eval()\n",
    "sublayer = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950b5e6-8958-46be-a058-16f8fe44aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe(emb(x))\n",
    "y = norm(x)\n",
    "y_res = residual_connection(x, sublayer)\n",
    "torch.all(y_res == (y+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7c83f-e211-4b23-8873-0feb354c00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.h = h\n",
    "    assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "    self.d_k = d_model // h\n",
    "    self.w_q = nn.Linear(d_model, d_model)\n",
    "    self.w_k = nn.Linear(d_model, d_model)\n",
    "    self.w_v = nn.Linear(d_model, d_model)\n",
    "    self.w_o = nn.Linear(d_model, d_model)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  @staticmethod\n",
    "  def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "    d_k = query.shape[-1]\n",
    "    # token to token attention, so matrix is seq_len, seq_len\n",
    "    # (batch, h, seq_len, d_k) -> (batch, h, seq_len, seq_len)\n",
    "    attention_scores = (query @ key.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "      # fill those masked location with large negative number so it softmaxes to zero\n",
    "      attention_scores.masked_fill(mask == 0, -1e9)\n",
    "    attention_scores = attention_scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
    "    if dropout is not None:\n",
    "      attention_scores = dropout(attention_scores)\n",
    "\n",
    "    return (attention_scores @ value), attention_scores\n",
    "\n",
    "  def forward(self, q, k, v, mask):\n",
    "    query = self.w_q(q) # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "    key = self.w_k(k)\n",
    "    value = self.w_v(v)\n",
    "\n",
    "    # 1) reshape q,k,v into separate heads\n",
    "    # 2) put the head dim as the 2nd dim\n",
    "    # 3) each head will see part of the embedding of ALL inputs in the batch\n",
    "    # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "    key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2) \n",
    "\n",
    "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "    # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k) -> (batch, seq_len, d_model)\n",
    "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.d_model)\n",
    "\n",
    "    # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "    return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06147b-8806-4bbd-8adc-c82aef9810d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedFowardBlock, dropout: float):\n",
    "    super().__init__()\n",
    "    self.self_attention_block = self_attention_block\n",
    "    self.feed_forward_block = feed_forward_block    \n",
    "    self.residual_connections = nn.ModuleList([\n",
    "      ResidualConnection(dropout) for _ in range(2)\n",
    "    ])\n",
    "\n",
    "  def forward(self, x, src_mask):\n",
    "    # src_mask for masking pad tokens\n",
    "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "    x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880300db-956f-4bab-8061-d9c7c262073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, layers: nn.ModuleList):\n",
    "    super().__init__()\n",
    "    self.layers = layers\n",
    "    self.norm = LayerNormalization()\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, mask)\n",
    "    return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ab673-7820-48ce-8671-71b0593ca141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedFowardBlock, dropout):\n",
    "    super().__init__()\n",
    "    self.self_attention_block = self_attention_block\n",
    "    self.cross_attention_block = cross_attention_block\n",
    "    self.feed_forward_block = feed_forward_block\n",
    "    self.residual_connections = nn.ModuleList([\n",
    "      ResidualConnection(dropout) for _ in range(3)\n",
    "    ])\n",
    "\n",
    "  def forward(self, x, encoder_output, src_mask, target_mask):\n",
    "    x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
    "    x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "    x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2683e35-e639-4336-9fef-a56f220c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, layers: nn.ModuleList):\n",
    "    super().__init__()\n",
    "    self.layers = layers\n",
    "    self.norm = LayerNormalization\n",
    "\n",
    "  def forward(self, x, encoder_output, src_mask, target_mask):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, encoder_output, src_mask, target_mask)\n",
    "    return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c28f98-e4ed-4f2a-b6e2-4f079667e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "  def __init__(self, d_model: int, vocab_size: int):\n",
    "    super().__init__()\n",
    "    self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # (batch, seq_len, d_model) -> (batch, seq_len, vocab_size)\n",
    "    return torch.log_softmax(self.proj(x), dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5e7bc-7396-4b2b-ab4c-74ca67765267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, enocder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, target_embed: InputEmbeddings, src_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.src_embed = src_embed\n",
    "    self.target_embed = target_embed\n",
    "    self.src_pos = src_pos\n",
    "    self.target_pos = target_pos\n",
    "    self.projection_layer = projection_layer\n",
    "\n",
    "  def encode(self, src, src_mask):\n",
    "    src = self.src_embd(src)\n",
    "    src = self.src_pos(src)\n",
    "    return self.encode(src, src_mask)\n",
    "\n",
    "  def decode(self, encoder_output, src_mask, target, target_mask):\n",
    "    target = self.target_embd(target)\n",
    "    target = self.target_pos(target)    \n",
    "    return self.decode(target, encoder_output, src_mask, target_mask)\n",
    "\n",
    "  def project(self, x):\n",
    "    return self.projection_layer(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d28b3-7945-4315-ac0a-aaa9793e4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, target_vocab_size: int, src_seq_len: int, target_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "  # embedding layers\n",
    "  src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "  target_embed = InputEmbeddings(d_model, target_vocab_size)\n",
    "  # positional encoding layers\n",
    "  src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "  target_pos = PositionalEncoding(d_model, target_seq_len, dropout)\n",
    "  # encoder blocks\n",
    "  encoder_blocks = []\n",
    "  for _ in range(N):\n",
    "    encoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "    feed_forward_block = FeedFowardBlock(d_model, d_ff, dropout)\n",
    "    encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "    encoder_blocks.append(encoder_block)\n",
    "    # decoder blocks\n",
    "  decoder_blocks = []\n",
    "  for _ in range(N):\n",
    "    decoder_self_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "    decoder_cross_attention_block = MultiHeadAttention(d_model, h, dropout)\n",
    "    feed_forward_block = FeedFowardBlock(d_model, d_ff, dropout)\n",
    "    decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "    decoder_blocks.append(decoder_block)\n",
    "\n",
    "  encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "  decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "  projection_layer = ProjectionLayer(d_model, target_vocab_size)\n",
    "\n",
    "  transformer = Transformer(encoder, decoder, src_embed, target_embed, src_pos, target_pos, projection_layer)\n",
    "\n",
    "  for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "      nn.init.xavier_uniform_(p)\n",
    "\n",
    "  return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29248821-5371-4beb-aaeb-3ce89fea837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, seq_len, h, d_model = 2, 6, 2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eacce3-2c9f-40a0-8aa9-af53a5f689c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_model // h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bab3b-e1cc-44f0-9b7b-4a8cd9137ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9189e6e-b51f-40f1-96d8-c402c817e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3e3c0-4531-42c0-93cd-0f37c0f8569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = w_q(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
